{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9582a54",
   "metadata": {},
   "source": [
    "# Tarea de Scraping Web con Python y Selenium\n",
    "\n",
    "## Objetivo\n",
    "Desarrollar habilidades prácticas en scraping web utilizando Python con Selenium y BeautifulSoup. Se espera que manejen el navegador de forma programática para extraer datos dinámicos de un sitio web específico y que realicen un análisis básico de los datos obtenidos.\n",
    "\n",
    "## Instrucciones\n",
    "1. Configura tu entorno de desarrollo instalando las librerías necesarias: Selenium, BeautifulSoup, pandas, entre otras que consideres necesarias.\n",
    "2. Configura un navegador usando Selenium. Debes asegurarte de incluir opciones como el modo incógnito y el modo sin interfaz gráfica (headless).\n",
    "3. Elige un sitio web que ofrezca datos dinámicos y sea legal para hacer scraping (por ejemplo, datos meteorológicos, precios de productos, cotizaciones de bolsa).\n",
    "4. Navega al sitio web utilizando Selenium, realiza búsquedas o filtra datos si es necesario utilizando interacciones del navegador.\n",
    "5. Extrae datos relevantes usando Selenium y BeautifulSoup. Debes obtener al menos tres tipos de datos relacionados (por ejemplo, nombre del producto, precio y categoría).\n",
    "6. Limpia y organiza los datos extraídos en un DataFrame de pandas.\n",
    "7. Realiza un análisis básico de los datos: puede ser estadístico descriptivo o alguna visualización simple.\n",
    "8. Documenta cada paso del proceso con comentarios en el código y celdas Markdown explicando las decisiones y métodos utilizados.\n",
    "\n",
    "## Criterios de Evaluación\n",
    "- Correcta configuración y uso de Selenium y BeautifulSoup.\n",
    "- Capacidad para navegar y extraer datos de forma efectiva y eficiente.\n",
    "- Limpieza y estructuración adecuada de los datos extraídos.\n",
    "- Calidad del análisis realizado y claridad en la documentación.\n",
    "\n",
    "## Entrega\n",
    "- Debes entregar este cuaderno Jupyter completado con todo el código, análisis y documentación solicitada.\n",
    "\n",
    "¡Buena suerte y que disfrutes del proceso de aprendizaje y exploración de datos!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d269a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el módulo time para manejar tiempos\n",
    "import time\n",
    "\n",
    "# Importar undetected_chromedriver como uc para evitar ser detectado\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "# Importar diferentes clases y métodos de Selenium para la automatización del navegador\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "# Importar BeautifulSoup (bs) para el análisis del código HTML\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# Importar el módulo requests para realizar solicitudes HTTP\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c74cf7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: Service /usr/bin/safaridriver unexpectedly exited. Status code was: 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Paso 2: Configurar el navegador con Selenium\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Inicializar el navegador Safari\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSafari\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.11/site-packages/selenium/webdriver/safari/webdriver.py:51\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, keep_alive, options, service)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m DriverFinder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice, options)\u001b[38;5;241m.\u001b[39mget_driver_path()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mreuse_service:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m executor \u001b[38;5;241m=\u001b[39m SafariRemoteConnection(\n\u001b[1;32m     54\u001b[0m     remote_server_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mservice_url,\n\u001b[1;32m     55\u001b[0m     keep_alive\u001b[38;5;241m=\u001b[39mkeep_alive,\n\u001b[1;32m     56\u001b[0m     ignore_proxy\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m_ignore_local_proxy,\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.11/site-packages/selenium/webdriver/common/service.py:102\u001b[0m, in \u001b[0;36mService.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_process_still_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connectable():\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.11/site-packages/selenium/webdriver/common/service.py:115\u001b[0m, in \u001b[0;36mService.assert_process_still_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m return_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code:\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WebDriverException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mService \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m unexpectedly exited. Status code was: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: Service /usr/bin/safaridriver unexpectedly exited. Status code was: 1\n"
     ]
    }
   ],
   "source": [
    "# Crear un objeto ChromeOptions para configurar las opciones del navegador\n",
    "options = uc.ChromeOptions()\n",
    "\n",
    "# Configurar el modo headless del navegador (sin interfaz gráfica)\n",
    "options.headless = True\n",
    "\n",
    "# Agregar argumentos para abrir una ventana de incógnito\n",
    "options.add_argument(\"--incognito\")\n",
    "\n",
    "# Agregar argumentos para deshabilitar las barras de información del navegador\n",
    "options.add_argument(\"--disable-infobars\")\n",
    "\n",
    "# Crear una instancia del navegador Chrome con las opciones configuradas\n",
    "driver = uc.Chrome(options=options)\n",
    "\n",
    "\n",
    "#options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.79 Safari/537.3\")\n",
    "#driver = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd02bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paso 3: Navegar al sitio web de Yahoo Finance\n",
    "url = 'https://finance.yahoo.com/quote/AAPL?p=AAPL'\n",
    "driver.get(url)\n",
    "time.sleep(3)  # Esperar a que la página cargue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9877aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paso 4: Extraer datos relevantes usando Selenium y BeautifulSoup\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Extraer datos: nombre del símbolo, precio actual y cambio porcentual\n",
    "nombre = soup.find('h1', {'data-reactid': '7'}).text\n",
    "precio_actual = soup.find('span', {'data-reactid': '32'}).text\n",
    "cambio_porcentual = soup.find('span', {'data-reactid': '33'}).text\n",
    "\n",
    "# Cerrar el navegador\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c353c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Paso 5: Limpiar y organizar los datos en un DataFrame de pandas\n",
    "data = {\n",
    "    'Nombre': [nombre],\n",
    "    'Precio Actual': [precio_actual],\n",
    "    'Cambio Porcentual': [cambio_porcentual]\n",
    "}\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1fd1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paso 6: Análisis básico de los datos\n",
    "# Descripción básica de los datos\n",
    "print(df.describe())\n",
    "\n",
    "# Visualización simple (si es necesario, instalar matplotlib: pip install matplotlib)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gráfica del precio actual (aunque solo tenemos un dato, es solo una demostración)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(df['Nombre'], df['Precio Actual'], color='blue')\n",
    "plt.xlabel('Nombre del Símbolo')\n",
    "plt.ylabel('Precio Actual')\n",
    "plt.title('Precio Actual de AAPL en Yahoo Finance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b736e812",
   "metadata": {},
   "source": [
    "## Paso 7: Documentación y comentarios\n",
    "\n",
    "\"\"\"\n",
    "### Documentación del Proceso\n",
    "\n",
    "1. **Importación de Bibliotecas**: Se importaron las bibliotecas necesarias para la automatización del navegador, análisis de datos y visualización.\n",
    "2. **Configuración del Navegador**: Se configuró el navegador Chrome en modo incógnito y sin interfaz gráfica (headless) para realizar el scraping de manera más eficiente y sin intervención del usuario.\n",
    "3. **Navegación al Sitio Web**: Utilizando Selenium, se navegó a la página de Yahoo Finance para obtener datos del símbolo de Apple (AAPL).\n",
    "4. **Extracción de Datos**: Se utilizó BeautifulSoup para analizar el HTML de la página y extraer el nombre del símbolo, el precio actual y el cambio porcentual.\n",
    "5. **Organización de Datos**: Los datos extraídos se almacenaron en un DataFrame de pandas para su posterior análisis.\n",
    "6. **Análisis Básico**: Se realizó un análisis descriptivo básico y una visualización simple del precio actual.\n",
    "\n",
    "### Decisiones y Métodos Utilizados\n",
    "\n",
    "- Se decidió utilizar Selenium debido a la naturaleza dinámica del contenido en Yahoo Finance.\n",
    "- BeautifulSoup se utilizó para facilitar la extracción de datos específicos del HTML.\n",
    "- Se incluyó una visualización simple para demostrar cómo se pueden representar los datos extraídos.\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "Este ejercicio demuestra cómo se pueden combinar Selenium y BeautifulSoup para realizar scraping web de sitios con contenido dinámico, organizar los datos con pandas y realizar análisis básicos.\n",
    "\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
